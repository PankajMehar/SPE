[param]

############################################
# training data config
############################################
# main work dir
root_dir = D:/dataset/icde2016/dataset
# the name of one dataset, such as facebook, linkedin
dataset_name = facebook
# the suffix of dataset, such as 10,100,1000
suffix = 10
# the relation name of data, such as classmateï¼Œfamily
class_name = classmate
# the index of the dataset file
index = 1

############################################
# offline results files 
############################################
# metagraph embedding path
metagraphEmbeddings_path = %(root_dir)s/metagraph-structural-similarity/%(dataset_name)s-metagraph.embedding
# words embeddings path
wordsEmbeddings_path = %(root_dir)s/%(dataset_name)s/vectorSaveFile
# sub-paths paths
subpaths_file = %(root_dir)s/%(dataset_name)s/newSubpathsSaveFile

############################################
# experiment parameters - do not need to change frequently
############################################
# the max length for sub-paths
maxlen_subpaths = 1000
# the size of words vocabulary
wordsSize = 1000000
# Sequence longer than this get ignored 
maxlen = 1000
# use a batch for training. This is the size of this batch.
batch_size = 10
# if need shuffle for training
is_shuffle_for_batch = True
# the frequences for display
dispFreq = 5
# the frequences for saving the parameters
saveFreq = 5
# the path for saving parameters. It is generated by main_dir, dataset_name, suffix, class_name and index. It will be generated in the code.
saveto = 
# the top num to predict
top_num = 10
# results file
result_save_file = %(root_dir)s/%(dataset_name)s.results/train.%(suffix)s/class_name

############################################
# experiment parameters - need to tune frequently
############################################
# learning rate
lrate = 0.0001
# metagraph embedding dimension 
metagraph_embedding_dimension = 32
# the dimension of attention when computing the m-node embedding
dimension_A = 12
# dimension of lstm parameters
dimension_lstm = 12
# the dimension of attention when computing the m-path embedding
dimension_B = 12
# the dimension of attention when computing the m-paths embedding
dimension_C = 12

# loss function, we use sigmoid here
objective_function_method = sigmoid
# the parameter in loss function, beta
objective_function_param = 0.1
# the max epochs for training
max_epochs = 100

# decay
decay_Q_A=0.00001
decay_b_A=0.00001
decay_eta_A=0.00001

decay_lstm_W = 0.00001
decay_lstm_U = 0.00001
decay_lstm_b = 0.00001

decay_Q_B=0.00001
decay_b_B=0.00001
decay_eta_B=0.00001

decay_Q_C=0.00001
decay_b_C=0.00001
decay_eta_C=0.00001

decay_w = 0.00001

